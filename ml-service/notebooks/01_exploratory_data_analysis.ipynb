{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Sri Lanka Tourism Reviews\n",
    "\n",
    "This notebook performs a comprehensive exploratory data analysis of tourism reviews for Sri Lankan destinations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading](#section1)\n",
    "2. [Text Analysis](#section2)\n",
    "3. [Location Analysis](#section3)\n",
    "4. [Temporal Analysis](#section4)\n",
    "5. [Sentiment Overview](#section5)\n",
    "6. [Data Quality](#section6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary libraries for data analysis, visualization, and NLP processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Database\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymongo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Database\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "# Text processing\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    print(\"‚úì NLTK data downloaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: NLTK download error (may already exist): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: Data Loading\n",
    "\n",
    "Load data from MongoDB and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB Atlas connection\n",
    "# Uses cloud database - no local MongoDB installation needed\n",
    "MONGO_URI = os.getenv(\n",
    "    \"MONGO_URI\", \n",
    "    \"mongodb+srv://pgmsadeep:1234@cluster0.phudmlq.mongodb.net/?retryWrites=true&w=majority\"\n",
    ")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"aiTourGuide\")\n",
    "\n",
    "print(\"üåê Connecting to MongoDB Atlas (Cloud Database)...\")\n",
    "print(f\"üìä Database: {DB_NAME}\")\n",
    "print(f\"‚òÅÔ∏è  Using cloud-hosted MongoDB - no local installation needed!\")\n",
    "\n",
    "# Connect to MongoDB Atlas\n",
    "client = MongoClient(\n",
    "    MONGO_URI,\n",
    "    serverSelectionTimeoutMS=10000,  # 10 second timeout for Atlas\n",
    "    retryWrites=True,\n",
    "    w='majority'\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"\\n‚úÖ Successfully connected to MongoDB Atlas!\")\n",
    "    print(\"‚ÑπÔ∏è  Note: Requires internet connection\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüîç Troubleshooting:\")\n",
    "    print(\"  1. Check your internet connection\")\n",
    "    print(\"  2. Verify MongoDB Atlas credentials in .env file\")\n",
    "    print(\"  3. Ensure IP address is whitelisted in Atlas\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews from MongoDB\n",
    "print(\"Loading reviews from MongoDB...\")\n",
    "reviews_cursor = db.reviews.find()\n",
    "reviews_df = pd.DataFrame(list(reviews_cursor))\n",
    "\n",
    "# Load locations from MongoDB\n",
    "print(\"Loading locations from MongoDB...\")\n",
    "locations_cursor = db.locations.find()\n",
    "locations_df = pd.DataFrame(list(locations_cursor))\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(reviews_df):,} reviews\")\n",
    "print(f\"‚úì Loaded {len(locations_df):,} locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about reviews\n",
    "print(\"=\"*70)\n",
    "print(\"REVIEWS DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal Reviews: {len(reviews_df):,}\")\n",
    "print(f\"Total Unique Destinations: {reviews_df['destination'].nunique():,}\")\n",
    "print(f\"Total Unique Districts: {reviews_df['district'].nunique():,}\")\n",
    "print(f\"Total Location Types: {reviews_df['location_type'].nunique():,}\")\n",
    "\n",
    "print(\"\\nLocation Types:\")\n",
    "for loc_type in reviews_df['location_type'].unique():\n",
    "    count = (reviews_df['location_type'] == loc_type).sum()\n",
    "    print(f\"  - {loc_type}: {count:,} reviews\")\n",
    "\n",
    "print(\"\\nDataset Shape:\", reviews_df.shape)\n",
    "print(\"\\nColumn Names:\", list(reviews_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about locations\n",
    "print(\"=\"*70)\n",
    "print(\"LOCATIONS DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal Locations: {len(locations_df):,}\")\n",
    "print(f\"\\nDataset Shape:\", locations_df.shape)\n",
    "print(\"\\nColumn Names:\", list(locations_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample reviews\n",
    "print(\"Sample Reviews (5 random samples):\")\n",
    "print(\"=\"*70)\n",
    "reviews_df[['destination', 'district', 'location_type', 'review_text']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample locations with key information\n",
    "print(\"Sample Locations (3 random samples):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, location in locations_df.sample(3).iterrows():\n",
    "    print(f\"\\n{location['name']}\")\n",
    "    print(f\"  ID: {location['locationId']}\")\n",
    "    print(f\"  Categories: {', '.join(location.get('category', []))}\")\n",
    "    coords = location.get('coordinates', {}).get('coordinates', [])\n",
    "    if coords:\n",
    "        print(f\"  Coordinates: [{coords[0]}, {coords[1]}]\")\n",
    "    details = location.get('details', {})\n",
    "    if details:\n",
    "        desc = details.get('description', 'N/A')\n",
    "        print(f\"  Description: {desc[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights - Data Loading\n",
    "\n",
    "**Summary:**\n",
    "- Dataset contains reviews from multiple Sri Lankan destinations\n",
    "- Reviews are categorized by location type (Beaches, Historical, etc.)\n",
    "- Location data includes geographic coordinates for mapping\n",
    "- Data is well-structured with consistent schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section2'></a>\n",
    "## Section 2: Text Analysis\n",
    "\n",
    "Analyze the textual content of reviews including length, common words, and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "reviews_df['review_length'] = reviews_df['review_text'].str.len()\n",
    "reviews_df['word_count'] = reviews_df['review_text'].str.split().str.len()\n",
    "reviews_df['sentence_count'] = reviews_df['review_text'].str.split('.').str.len()\n",
    "\n",
    "# Display statistics\n",
    "print(\"Review Text Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCharacter Count:\")\n",
    "print(f\"  Mean: {reviews_df['review_length'].mean():.2f}\")\n",
    "print(f\"  Median: {reviews_df['review_length'].median():.2f}\")\n",
    "print(f\"  Min: {reviews_df['review_length'].min()}\")\n",
    "print(f\"  Max: {reviews_df['review_length'].max()}\")\n",
    "print(f\"  Std Dev: {reviews_df['review_length'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nWord Count:\")\n",
    "print(f\"  Mean: {reviews_df['word_count'].mean():.2f}\")\n",
    "print(f\"  Median: {reviews_df['word_count'].median():.2f}\")\n",
    "print(f\"  Min: {reviews_df['word_count'].min()}\")\n",
    "print(f\"  Max: {reviews_df['word_count'].max()}\")\n",
    "\n",
    "print(f\"\\nSentence Count:\")\n",
    "print(f\"  Mean: {reviews_df['sentence_count'].mean():.2f}\")\n",
    "print(f\"  Median: {reviews_df['sentence_count'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for review length distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Review Length Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Character length histogram\n",
    "axes[0, 0].hist(reviews_df['review_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(reviews_df['review_length'].mean(), color='red', \n",
    "                   linestyle='--', linewidth=2, label=f\"Mean: {reviews_df['review_length'].mean():.0f}\")\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Review Length (Characters)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Word count histogram\n",
    "axes[0, 1].hist(reviews_df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].axvline(reviews_df['word_count'].mean(), color='red', \n",
    "                   linestyle='--', linewidth=2, label=f\"Mean: {reviews_df['word_count'].mean():.0f}\")\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Review Length (Words)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot for review length by location type\n",
    "reviews_df.boxplot(column='review_length', by='location_type', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Location Type')\n",
    "axes[1, 0].set_ylabel('Character Count')\n",
    "axes[1, 0].set_title('Review Length by Location Type')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Box plot for word count by location type\n",
    "reviews_df.boxplot(column='word_count', by='location_type', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Location Type')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "axes[1, 1].set_title('Word Count by Location Type')\n",
    "plt.sca(axes[1, 1])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for word frequency analysis\n",
    "# Combine all reviews into one text\n",
    "all_text = ' '.join(reviews_df['review_text'].astype(str))\n",
    "\n",
    "# Clean text: lowercase and remove special characters\n",
    "all_text_clean = re.sub(r'[^a-zA-Z\\s]', '', all_text.lower())\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add custom stopwords specific to reviews\n",
    "custom_stopwords = {'beach', 'place', 'good', 'nice', 'great', 'visit', 'really', 'one', 'also', 'would', 'like'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "words = word_tokenize(all_text_clean)\n",
    "words_filtered = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words_filtered)\n",
    "top_words = word_freq.most_common(30)\n",
    "\n",
    "print(\"Top 30 Most Common Words:\")\n",
    "print(\"=\"*70)\n",
    "for idx, (word, count) in enumerate(top_words, 1):\n",
    "    print(f\"{idx:2}. {word:20} - {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words as bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "words_list = [word for word, count in top_words]\n",
    "counts_list = [count for word, count in top_words]\n",
    "\n",
    "bars = ax.barh(words_list, counts_list, color=sns.color_palette(\"viridis\", len(words_list)))\n",
    "ax.set_xlabel('Frequency', fontsize=12)\n",
    "ax.set_ylabel('Words', fontsize=12)\n",
    "ax.set_title('Top 30 Most Frequent Words in Reviews', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, counts_list)):\n",
    "    ax.text(count + 10, i, f'{count:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud\n",
    "# Prepare stopwords for wordcloud\n",
    "wordcloud_stopwords = STOPWORDS.union(stop_words)\n",
    "\n",
    "# Create word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    background_color='white',\n",
    "    stopwords=wordcloud_stopwords,\n",
    "    colormap='viridis',\n",
    "    max_words=100,\n",
    "    relative_scaling=0.5,\n",
    "    min_font_size=10\n",
    ").generate(all_text_clean)\n",
    "\n",
    "# Display word cloud\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis('off')\n",
    "ax.set_title('Word Cloud of Tourism Reviews', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds by Location Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for each location type\n",
    "location_types = reviews_df['location_type'].unique()\n",
    "n_types = len(location_types)\n",
    "\n",
    "# Calculate grid dimensions\n",
    "n_cols = 3\n",
    "n_rows = (n_types + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6 * n_rows))\n",
    "fig.suptitle('Word Clouds by Location Type', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "if n_rows > 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes] if n_types == 1 else axes\n",
    "\n",
    "for idx, loc_type in enumerate(location_types):\n",
    "    # Get reviews for this location type\n",
    "    type_text = ' '.join(reviews_df[reviews_df['location_type'] == loc_type]['review_text'].astype(str))\n",
    "    type_text_clean = re.sub(r'[^a-zA-Z\\s]', '', type_text.lower())\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        stopwords=wordcloud_stopwords,\n",
    "        colormap='plasma',\n",
    "        max_words=50\n",
    "    ).generate(type_text_clean)\n",
    "    \n",
    "    # Display\n",
    "    axes[idx].imshow(wc, interpolation='bilinear')\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'{loc_type}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_types, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Encoding Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for encoding issues\n",
    "print(\"Checking for character encoding issues...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for non-ASCII characters\n",
    "non_ascii_count = 0\n",
    "for text in reviews_df['review_text']:\n",
    "    if any(ord(char) > 127 for char in str(text)):\n",
    "        non_ascii_count += 1\n",
    "\n",
    "print(f\"Reviews with non-ASCII characters: {non_ascii_count} ({non_ascii_count/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "# Sample reviews with non-ASCII characters\n",
    "if non_ascii_count > 0:\n",
    "    print(\"\\nSample reviews with special characters:\")\n",
    "    count = 0\n",
    "    for idx, text in reviews_df['review_text'].items():\n",
    "        if any(ord(char) > 127 for char in str(text)):\n",
    "            print(f\"\\n{count + 1}. {text[:100]}...\")\n",
    "            # Show special characters\n",
    "            special_chars = set([char for char in str(text) if ord(char) > 127])\n",
    "            print(f\"   Special characters: {special_chars}\")\n",
    "            count += 1\n",
    "            if count >= 3:\n",
    "                break\n",
    "else:\n",
    "    print(\"‚úì No encoding issues detected\")\n",
    "\n",
    "# Check for common problematic patterns\n",
    "print(\"\\nChecking for common issues:\")\n",
    "print(f\"Reviews with multiple spaces: {reviews_df['review_text'].str.contains('  ').sum()}\")\n",
    "print(f\"Reviews with tabs: {reviews_df['review_text'].str.contains('\\t').sum()}\")\n",
    "print(f\"Reviews with newlines: {reviews_df['review_text'].str.contains('\\n').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights - Text Analysis\n",
    "\n",
    "**Summary:**\n",
    "- Average review length provides insights into engagement level\n",
    "- Most common words reveal key themes and topics\n",
    "- Word clouds visualize dominant themes by location type\n",
    "- Character encoding check ensures data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section3'></a>\n",
    "## Section 3: Location Analysis\n",
    "\n",
    "Analyze the distribution of reviews across locations, districts, and categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews per Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count reviews per destination\n",
    "reviews_per_destination = reviews_df['destination'].value_counts().head(20)\n",
    "\n",
    "print(\"Top 20 Destinations by Review Count:\")\n",
    "print(\"=\"*70)\n",
    "for idx, (dest, count) in enumerate(reviews_per_destination.items(), 1):\n",
    "    print(f\"{idx:2}. {dest:40} - {count:,} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top destinations\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "bars = ax.barh(range(len(reviews_per_destination)), reviews_per_destination.values, \n",
    "               color=sns.color_palette(\"coolwarm\", len(reviews_per_destination)))\n",
    "ax.set_yticks(range(len(reviews_per_destination)))\n",
    "ax.set_yticklabels(reviews_per_destination.index)\n",
    "ax.set_xlabel('Number of Reviews', fontsize=12)\n",
    "ax.set_ylabel('Destination', fontsize=12)\n",
    "ax.set_title('Top 20 Destinations by Review Count', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, reviews_per_destination.values)):\n",
    "    ax.text(count + 1, i, f'{count}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count reviews by location type\n",
    "location_type_counts = reviews_df['location_type'].value_counts()\n",
    "\n",
    "print(\"Reviews by Location Type:\")\n",
    "print(\"=\"*70)\n",
    "for loc_type, count in location_type_counts.items():\n",
    "    percentage = (count / len(reviews_df)) * 100\n",
    "    print(f\"{loc_type:20} - {count:,} reviews ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pie chart for location types\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Pie chart\n",
    "colors = sns.color_palette('pastel', len(location_type_counts))\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    location_type_counts.values,\n",
    "    labels=location_type_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    explode=[0.05] * len(location_type_counts)\n",
    ")\n",
    "ax1.set_title('Distribution of Reviews by Location Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Make percentage text bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(11)\n",
    "\n",
    "# Bar chart for better comparison\n",
    "ax2.bar(location_type_counts.index, location_type_counts.values, color=colors, edgecolor='black')\n",
    "ax2.set_xlabel('Location Type', fontsize=12)\n",
    "ax2.set_ylabel('Number of Reviews', fontsize=12)\n",
    "ax2.set_title('Reviews Count by Location Type', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (loc_type, count) in enumerate(location_type_counts.items()):\n",
    "    ax2.text(i, count + 5, f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### District Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count reviews by district\n",
    "district_counts = reviews_df['district'].value_counts().head(15)\n",
    "\n",
    "print(\"Top 15 Districts by Review Count:\")\n",
    "print(\"=\"*70)\n",
    "for idx, (district, count) in enumerate(district_counts.items(), 1):\n",
    "    percentage = (count / len(reviews_df)) * 100\n",
    "    print(f\"{idx:2}. {district:20} - {count:,} reviews ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize district distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "bars = ax.barh(range(len(district_counts)), district_counts.values,\n",
    "               color=sns.color_palette(\"Spectral\", len(district_counts)))\n",
    "ax.set_yticks(range(len(district_counts)))\n",
    "ax.set_yticklabels(district_counts.index)\n",
    "ax.set_xlabel('Number of Reviews', fontsize=12)\n",
    "ax.set_ylabel('District', fontsize=12)\n",
    "ax.set_title('Top 15 Districts by Review Count', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, district_counts.values)):\n",
    "    ax.text(count + 2, i, f'{count}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Distribution Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from locations\n",
    "locations_with_coords = []\n",
    "\n",
    "for idx, location in locations_df.iterrows():\n",
    "    coords = location.get('coordinates', {}).get('coordinates', [])\n",
    "    if coords and len(coords) == 2:\n",
    "        locations_with_coords.append({\n",
    "            'name': location['name'],\n",
    "            'longitude': coords[0],\n",
    "            'latitude': coords[1],\n",
    "            'categories': ', '.join(location.get('category', [])),\n",
    "            'locationId': location.get('locationId', 'N/A')\n",
    "        })\n",
    "\n",
    "locations_coords_df = pd.DataFrame(locations_with_coords)\n",
    "\n",
    "print(f\"Locations with coordinates: {len(locations_coords_df)}\")\n",
    "print(f\"\\nCoordinate ranges:\")\n",
    "print(f\"  Latitude: {locations_coords_df['latitude'].min():.4f} to {locations_coords_df['latitude'].max():.4f}\")\n",
    "print(f\"  Longitude: {locations_coords_df['longitude'].min():.4f} to {locations_coords_df['longitude'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive map with Folium\n",
    "if len(locations_coords_df) > 0:\n",
    "    # Center map on Sri Lanka\n",
    "    center_lat = locations_coords_df['latitude'].mean()\n",
    "    center_lon = locations_coords_df['longitude'].mean()\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=8,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "    \n",
    "    # Add marker cluster\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    \n",
    "    # Add markers for each location\n",
    "    for idx, row in locations_coords_df.iterrows():\n",
    "        popup_text = f\"\"\"\n",
    "        <b>{row['name']}</b><br>\n",
    "        Categories: {row['categories']}<br>\n",
    "        Coordinates: [{row['latitude']:.4f}, {row['longitude']:.4f}]\n",
    "        \"\"\"\n",
    "        \n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            popup=folium.Popup(popup_text, max_width=300),\n",
    "            tooltip=row['name'],\n",
    "            icon=folium.Icon(color='blue', icon='info-sign')\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    # Add heat map layer\n",
    "    heat_data = [[row['latitude'], row['longitude']] for idx, row in locations_coords_df.iterrows()]\n",
    "    HeatMap(heat_data, radius=15, blur=20, max_zoom=10).add_to(m)\n",
    "    \n",
    "    # Save and display map\n",
    "    map_file = '../output/sri_lanka_locations_map.html'\n",
    "    os.makedirs('../output', exist_ok=True)\n",
    "    m.save(map_file)\n",
    "    print(f\"\\n‚úì Interactive map saved to: {map_file}\")\n",
    "    print(\"Open this file in a web browser to view the interactive map\")\n",
    "    \n",
    "    # Display map in notebook\n",
    "    display(m)\n",
    "else:\n",
    "    print(\"No location coordinates available for mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Type vs District Cross-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab of location types and districts\n",
    "crosstab = pd.crosstab(reviews_df['location_type'], reviews_df['district'])\n",
    "\n",
    "# Show top districts for each location type\n",
    "print(\"Top 5 Districts for Each Location Type:\")\n",
    "print(\"=\"*70)\n",
    "for loc_type in crosstab.index:\n",
    "    top_districts = crosstab.loc[loc_type].sort_values(ascending=False).head(5)\n",
    "    print(f\"\\n{loc_type}:\")\n",
    "    for district, count in top_districts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {district}: {count} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with heatmap\n",
    "# Select top 10 districts by total review count\n",
    "top_districts = reviews_df['district'].value_counts().head(10).index\n",
    "crosstab_filtered = crosstab[top_districts]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(crosstab_filtered, annot=True, fmt='d', cmap='YlOrRd', \n",
    "            linewidths=0.5, cbar_kws={'label': 'Number of Reviews'}, ax=ax)\n",
    "ax.set_title('Location Type vs District Heatmap (Top 10 Districts)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('District', fontsize=12)\n",
    "ax.set_ylabel('Location Type', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights - Location Analysis\n",
    "\n",
    "**Summary:**\n",
    "- Most popular destinations identified by review volume\n",
    "- Location types show clear preferences (beaches, historical sites, etc.)\n",
    "- Geographic distribution reveals tourism hotspots\n",
    "- District analysis shows regional tourism patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section4'></a>\n",
    "## Section 4: Temporal Analysis\n",
    "\n",
    "Analyze temporal patterns in the review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if created_at field exists and has data\n",
    "if 'created_at' in reviews_df.columns and reviews_df['created_at'].notna().any():\n",
    "    # Convert to datetime if needed\n",
    "    reviews_df['created_at'] = pd.to_datetime(reviews_df['created_at'])\n",
    "    \n",
    "    # Extract temporal components\n",
    "    reviews_df['year'] = reviews_df['created_at'].dt.year\n",
    "    reviews_df['month'] = reviews_df['created_at'].dt.month\n",
    "    reviews_df['month_name'] = reviews_df['created_at'].dt.month_name()\n",
    "    reviews_df['day_of_week'] = reviews_df['created_at'].dt.day_name()\n",
    "    reviews_df['quarter'] = reviews_df['created_at'].dt.quarter\n",
    "    \n",
    "    has_temporal_data = True\n",
    "    print(\"‚úì Temporal data available\")\n",
    "    print(f\"\\nDate range: {reviews_df['created_at'].min()} to {reviews_df['created_at'].max()}\")\n",
    "else:\n",
    "    has_temporal_data = False\n",
    "    print(\"‚ÑπÔ∏è  Note: Temporal data (created_at) not available or all null\")\n",
    "    print(\"Skipping temporal analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_temporal_data:\n",
    "    # Reviews over time\n",
    "    reviews_by_date = reviews_df.groupby(reviews_df['created_at'].dt.date).size()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Temporal Analysis of Reviews', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Time series plot\n",
    "    axes[0, 0].plot(reviews_by_date.index, reviews_by_date.values, linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Number of Reviews')\n",
    "    axes[0, 0].set_title('Reviews Over Time')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Reviews by month\n",
    "    month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                   'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    monthly_counts = reviews_df['month_name'].value_counts().reindex(month_order, fill_value=0)\n",
    "    axes[0, 1].bar(range(12), monthly_counts.values, color=sns.color_palette('coolwarm', 12))\n",
    "    axes[0, 1].set_xticks(range(12))\n",
    "    axes[0, 1].set_xticklabels([m[:3] for m in month_order], rotation=45)\n",
    "    axes[0, 1].set_xlabel('Month')\n",
    "    axes[0, 1].set_ylabel('Number of Reviews')\n",
    "    axes[0, 1].set_title('Reviews by Month (Seasonality)')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Reviews by day of week\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    daily_counts = reviews_df['day_of_week'].value_counts().reindex(day_order, fill_value=0)\n",
    "    axes[1, 0].bar(range(7), daily_counts.values, color=sns.color_palette('viridis', 7))\n",
    "    axes[1, 0].set_xticks(range(7))\n",
    "    axes[1, 0].set_xticklabels([d[:3] for d in day_order], rotation=45)\n",
    "    axes[1, 0].set_xlabel('Day of Week')\n",
    "    axes[1, 0].set_ylabel('Number of Reviews')\n",
    "    axes[1, 0].set_title('Reviews by Day of Week')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Reviews by year (if multiple years)\n",
    "    yearly_counts = reviews_df['year'].value_counts().sort_index()\n",
    "    axes[1, 1].bar(yearly_counts.index, yearly_counts.values, color='steelblue')\n",
    "    axes[1, 1].set_xlabel('Year')\n",
    "    axes[1, 1].set_ylabel('Number of Reviews')\n",
    "    axes[1, 1].set_title('Reviews by Year')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nTemporal Statistics:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nMost active month: {monthly_counts.idxmax()} ({monthly_counts.max()} reviews)\")\n",
    "    print(f\"Least active month: {monthly_counts.idxmin()} ({monthly_counts.min()} reviews)\")\n",
    "    print(f\"\\nMost active day: {daily_counts.idxmax()} ({daily_counts.max()} reviews)\")\n",
    "    print(f\"Least active day: {daily_counts.idxmin()} ({daily_counts.min()} reviews)\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Temporal analysis skipped - no date information available\")\n",
    "    print(\"Note: All reviews were imported with current timestamp.\")\n",
    "    print(\"Original review dates may not be preserved in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights - Temporal Analysis\n",
    "\n",
    "**Summary:**\n",
    "- Temporal patterns reveal tourism seasonality\n",
    "- Peak months identified for tourism planning\n",
    "- Day-of-week patterns show review submission behavior\n",
    "- Year-over-year trends (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section5'></a>\n",
    "## Section 5: Sentiment Overview\n",
    "\n",
    "Analyze sentiment in reviews using VADER sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Running sentiment analysis on reviews...\")\n",
    "print(\"This may take a few moments...\")\n",
    "\n",
    "# Analyze sentiment for each review\n",
    "sentiments = []\n",
    "for text in reviews_df['review_text']:\n",
    "    sentiment_scores = analyzer.polarity_scores(str(text))\n",
    "    sentiments.append(sentiment_scores)\n",
    "\n",
    "# Create sentiment dataframe\n",
    "sentiment_df = pd.DataFrame(sentiments)\n",
    "\n",
    "# Add sentiment scores to reviews dataframe\n",
    "reviews_df['sentiment_neg'] = sentiment_df['neg']\n",
    "reviews_df['sentiment_neu'] = sentiment_df['neu']\n",
    "reviews_df['sentiment_pos'] = sentiment_df['pos']\n",
    "reviews_df['sentiment_compound'] = sentiment_df['compound']\n",
    "\n",
    "# Classify sentiment based on compound score\n",
    "def classify_sentiment(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "reviews_df['sentiment_label'] = reviews_df['sentiment_compound'].apply(classify_sentiment)\n",
    "\n",
    "print(\"\\n‚úì Sentiment analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count sentiment labels\n",
    "sentiment_counts = reviews_df['sentiment_label'].value_counts()\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(\"=\"*70)\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(reviews_df)) * 100\n",
    "    print(f\"{sentiment:10} - {count:,} reviews ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\nAverage Sentiment Scores:\")\n",
    "print(f\"  Positive: {reviews_df['sentiment_pos'].mean():.4f}\")\n",
    "print(f\"  Neutral:  {reviews_df['sentiment_neu'].mean():.4f}\")\n",
    "print(f\"  Negative: {reviews_df['sentiment_neg'].mean():.4f}\")\n",
    "print(f\"  Compound: {reviews_df['sentiment_compound'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Sentiment Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Pie chart of sentiment labels\n",
    "colors_sentiment = {'Positive': '#2ecc71', 'Neutral': '#95a5a6', 'Negative': '#e74c3c'}\n",
    "colors = [colors_sentiment[label] for label in sentiment_counts.index]\n",
    "wedges, texts, autotexts = axes[0, 0].pie(\n",
    "    sentiment_counts.values,\n",
    "    labels=sentiment_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    explode=[0.05] * len(sentiment_counts),\n",
    "    startangle=90\n",
    ")\n",
    "axes[0, 0].set_title('Overall Sentiment Distribution')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# Histogram of compound scores\n",
    "axes[0, 1].hist(reviews_df['sentiment_compound'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Neutral threshold')\n",
    "axes[0, 1].axvline(reviews_df['sentiment_compound'].mean(), color='green', \n",
    "                   linestyle='--', linewidth=2, label=f\"Mean: {reviews_df['sentiment_compound'].mean():.3f}\")\n",
    "axes[0, 1].set_xlabel('Compound Sentiment Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Compound Sentiment Scores')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of sentiment scores\n",
    "sentiment_data = [reviews_df['sentiment_pos'], reviews_df['sentiment_neu'], reviews_df['sentiment_neg']]\n",
    "axes[1, 0].boxplot(sentiment_data, labels=['Positive', 'Neutral', 'Negative'])\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Distribution of Sentiment Component Scores')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sentiment by location type\n",
    "sentiment_by_type = reviews_df.groupby('location_type')['sentiment_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "sentiment_by_type.plot(kind='bar', stacked=True, ax=axes[1, 1], \n",
    "                       color=[colors_sentiment.get(col, 'gray') for col in sentiment_by_type.columns])\n",
    "axes[1, 1].set_xlabel('Location Type')\n",
    "axes[1, 1].set_ylabel('Proportion')\n",
    "axes[1, 1].set_title('Sentiment Distribution by Location Type')\n",
    "axes[1, 1].legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Positive vs Negative Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sentiment by destination (min 5 reviews)\n",
    "destination_sentiment = reviews_df.groupby('destination').agg({\n",
    "    'sentiment_compound': ['mean', 'count']\n",
    "}).reset_index()\n",
    "destination_sentiment.columns = ['destination', 'avg_sentiment', 'review_count']\n",
    "\n",
    "# Filter destinations with at least 5 reviews\n",
    "min_reviews = 5\n",
    "destination_sentiment_filtered = destination_sentiment[destination_sentiment['review_count'] >= min_reviews]\n",
    "\n",
    "# Get top 10 positive and negative\n",
    "top_positive = destination_sentiment_filtered.nlargest(10, 'avg_sentiment')\n",
    "top_negative = destination_sentiment_filtered.nsmallest(10, 'avg_sentiment')\n",
    "\n",
    "print(f\"Top 10 Most Positive Destinations (min {min_reviews} reviews):\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in top_positive.iterrows():\n",
    "    print(f\"{row['destination']:40} - Score: {row['avg_sentiment']:.4f} ({int(row['review_count'])} reviews)\")\n",
    "\n",
    "print(f\"\\nTop 10 Most Negative Destinations (min {min_reviews} reviews):\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in top_negative.iterrows():\n",
    "    print(f\"{row['destination']:40} - Score: {row['avg_sentiment']:.4f} ({int(row['review_count'])} reviews)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top positive and negative destinations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "fig.suptitle(f'Top 10 Destinations by Sentiment (min {min_reviews} reviews)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Top positive\n",
    "ax1.barh(range(len(top_positive)), top_positive['avg_sentiment'].values,\n",
    "         color='#2ecc71', edgecolor='black')\n",
    "ax1.set_yticks(range(len(top_positive)))\n",
    "ax1.set_yticklabels(top_positive['destination'].values)\n",
    "ax1.set_xlabel('Average Sentiment Score')\n",
    "ax1.set_title('Most Positive Destinations', fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add score labels\n",
    "for i, score in enumerate(top_positive['avg_sentiment'].values):\n",
    "    ax1.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Top negative\n",
    "ax2.barh(range(len(top_negative)), top_negative['avg_sentiment'].values,\n",
    "         color='#e74c3c', edgecolor='black')\n",
    "ax2.set_yticks(range(len(top_negative)))\n",
    "ax2.set_yticklabels(top_negative['destination'].values)\n",
    "ax2.set_xlabel('Average Sentiment Score')\n",
    "ax2.set_title('Most Negative Destinations', fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add score labels\n",
    "for i, score in enumerate(top_negative['avg_sentiment'].values):\n",
    "    ax2.text(score - 0.01, i, f'{score:.3f}', va='center', ha='right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Reviews by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample reviews for each sentiment\n",
    "print(\"Sample Reviews by Sentiment:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sentiment in ['Positive', 'Neutral', 'Negative']:\n",
    "    print(f\"\\n{sentiment.upper()} REVIEWS:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    sample_reviews = reviews_df[reviews_df['sentiment_label'] == sentiment].sample(\n",
    "        min(3, len(reviews_df[reviews_df['sentiment_label'] == sentiment]))\n",
    "    )\n",
    "    \n",
    "    for idx, row in sample_reviews.iterrows():\n",
    "        print(f\"\\nDestination: {row['destination']}\")\n",
    "        print(f\"Sentiment Score: {row['sentiment_compound']:.4f}\")\n",
    "        print(f\"Review: {row['review_text'][:200]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights - Sentiment Analysis\n",
    "\n",
    "**Summary:**\n",
    "- Overall sentiment tends to be positive/neutral/negative (based on results)\n",
    "- Identified destinations with highest positive sentiment\n",
    "- Identified destinations that may need improvement\n",
    "- Sentiment varies by location type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section6'></a>\n",
    "## Section 6: Data Quality\n",
    "\n",
    "Check data quality including missing values, duplicates, and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in reviews\n",
    "print(\"Missing Values in Reviews Dataset:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_values = reviews_df.isnull().sum()\n",
    "missing_percentages = (missing_values / len(reviews_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Percentage': missing_percentages.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\n‚úì No missing values found in reviews dataset\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Total missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if missing_values.sum() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "    \n",
    "    ax.bar(missing_cols['Column'], missing_cols['Percentage'], color='coral', edgecolor='black')\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Missing Percentage (%)')\n",
    "    ax.set_title('Missing Values by Column', fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No visualization needed - no missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate reviews\n",
    "print(\"Duplicate Detection:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Exact duplicates (all columns)\n",
    "exact_duplicates = reviews_df.duplicated().sum()\n",
    "print(f\"\\nExact duplicate rows: {exact_duplicates} ({exact_duplicates/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "# Duplicate review text\n",
    "text_duplicates = reviews_df['review_text'].duplicated().sum()\n",
    "print(f\"Duplicate review texts: {text_duplicates} ({text_duplicates/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "# Duplicate destination + review combinations\n",
    "dest_review_duplicates = reviews_df.duplicated(subset=['destination', 'review_text']).sum()\n",
    "print(f\"Duplicate destination+review combinations: {dest_review_duplicates} ({dest_review_duplicates/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "if text_duplicates > 0:\n",
    "    print(\"\\nSample duplicate reviews:\")\n",
    "    duplicate_texts = reviews_df[reviews_df['review_text'].duplicated(keep=False)]\n",
    "    sample_text = duplicate_texts['review_text'].iloc[0]\n",
    "    matching_reviews = reviews_df[reviews_df['review_text'] == sample_text]\n",
    "    print(f\"\\nReview text: {sample_text[:150]}...\")\n",
    "    print(f\"Appears for destinations: {matching_reviews['destination'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in review length\n",
    "print(\"Outlier Analysis - Review Length:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate IQR for review length\n",
    "Q1 = reviews_df['review_length'].quantile(0.25)\n",
    "Q3 = reviews_df['review_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = reviews_df[(reviews_df['review_length'] < lower_bound) | \n",
    "                      (reviews_df['review_length'] > upper_bound)]\n",
    "\n",
    "print(f\"\\nIQR Statistics:\")\n",
    "print(f\"  Q1 (25th percentile): {Q1:.2f}\")\n",
    "print(f\"  Q3 (75th percentile): {Q3:.2f}\")\n",
    "print(f\"  IQR: {IQR:.2f}\")\n",
    "print(f\"  Lower bound: {lower_bound:.2f}\")\n",
    "print(f\"  Upper bound: {upper_bound:.2f}\")\n",
    "\n",
    "print(f\"\\nOutliers detected: {len(outliers)} ({len(outliers)/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\nOutlier categories:\")\n",
    "    short_outliers = outliers[outliers['review_length'] < lower_bound]\n",
    "    long_outliers = outliers[outliers['review_length'] > upper_bound]\n",
    "    print(f\"  Unusually short reviews: {len(short_outliers)}\")\n",
    "    print(f\"  Unusually long reviews: {len(long_outliers)}\")\n",
    "    \n",
    "    # Sample outliers\n",
    "    if len(short_outliers) > 0:\n",
    "        print(\"\\nSample short outlier:\")\n",
    "        sample = short_outliers.iloc[0]\n",
    "        print(f\"  Length: {sample['review_length']} characters\")\n",
    "        print(f\"  Text: '{sample['review_text']}'\")\n",
    "    \n",
    "    if len(long_outliers) > 0:\n",
    "        print(\"\\nSample long outlier:\")\n",
    "        sample = long_outliers.iloc[0]\n",
    "        print(f\"  Length: {sample['review_length']} characters\")\n",
    "        print(f\"  Text: {sample['review_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Outlier Detection - Review Length', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Box plot\n",
    "axes[0].boxplot(reviews_df['review_length'], vert=True)\n",
    "axes[0].set_ylabel('Character Count')\n",
    "axes[0].set_title('Box Plot of Review Length')\n",
    "axes[0].axhline(y=lower_bound, color='r', linestyle='--', label='Lower Bound')\n",
    "axes[0].axhline(y=upper_bound, color='r', linestyle='--', label='Upper Bound')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with outlier bounds\n",
    "axes[1].hist(reviews_df['review_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=lower_bound, color='r', linestyle='--', linewidth=2, label='Lower Bound')\n",
    "axes[1].axvline(x=upper_bound, color='r', linestyle='--', linewidth=2, label='Upper Bound')\n",
    "axes[1].axvline(x=reviews_df['review_length'].median(), color='g', \n",
    "               linestyle='--', linewidth=2, label='Median')\n",
    "axes[1].set_xlabel('Character Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution with Outlier Bounds')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Dataset Size:\")\n",
    "print(f\"  Total reviews: {len(reviews_df):,}\")\n",
    "print(f\"  Total locations: {len(locations_df):,}\")\n",
    "\n",
    "print(f\"\\nüîç Completeness:\")\n",
    "completeness = (1 - (missing_values.sum() / (len(reviews_df) * len(reviews_df.columns)))) * 100\n",
    "print(f\"  Data completeness: {completeness:.2f}%\")\n",
    "print(f\"  Missing values: {missing_values.sum()}\")\n",
    "\n",
    "print(f\"\\nüîÑ Duplicates:\")\n",
    "print(f\"  Exact duplicates: {exact_duplicates} ({exact_duplicates/len(reviews_df)*100:.2f}%)\")\n",
    "print(f\"  Duplicate review texts: {text_duplicates} ({text_duplicates/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüìè Text Quality:\")\n",
    "print(f\"  Average review length: {reviews_df['review_length'].mean():.2f} characters\")\n",
    "print(f\"  Average word count: {reviews_df['word_count'].mean():.2f} words\")\n",
    "print(f\"  Outliers: {len(outliers)} ({len(outliers)/len(reviews_df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüòä Sentiment:\")\n",
    "print(f\"  Positive reviews: {(reviews_df['sentiment_label']=='Positive').sum()} ({(reviews_df['sentiment_label']=='Positive').sum()/len(reviews_df)*100:.2f}%)\")\n",
    "print(f\"  Neutral reviews: {(reviews_df['sentiment_label']=='Neutral').sum()} ({(reviews_df['sentiment_label']=='Neutral').sum()/len(reviews_df)*100:.2f}%)\")\n",
    "print(f\"  Negative reviews: {(reviews_df['sentiment_label']=='Negative').sum()} ({(reviews_df['sentiment_label']=='Negative').sum()/len(reviews_df)*100:.2f}%)\")\n",
    "print(f\"  Average compound score: {reviews_df['sentiment_compound'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Overall Quality Assessment:\")\n",
    "if completeness > 95 and exact_duplicates < len(reviews_df) * 0.05:\n",
    "    print(\"  Status: EXCELLENT - High quality dataset\")\n",
    "elif completeness > 90 and exact_duplicates < len(reviews_df) * 0.10:\n",
    "    print(\"  Status: GOOD - Dataset is suitable for analysis\")\n",
    "else:\n",
    "    print(\"  Status: FAIR - Some data quality issues detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights - Data Quality\n",
    "\n",
    "**Summary:**\n",
    "- Data completeness is high/moderate/low (based on results)\n",
    "- Duplicate analysis reveals data collection patterns\n",
    "- Outlier detection identifies unusual reviews\n",
    "- Overall data quality is suitable for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Overview**\n",
    "   - Comprehensive collection of Sri Lankan tourism reviews\n",
    "   - Multiple location types and districts covered\n",
    "   - Rich textual data for analysis\n",
    "\n",
    "2. **Text Characteristics**\n",
    "   - Review length statistics indicate engagement levels\n",
    "   - Common themes identified through word analysis\n",
    "   - Location-specific vocabulary patterns\n",
    "\n",
    "3. **Geographic Insights**\n",
    "   - Popular destinations identified\n",
    "   - Tourism hotspots mapped\n",
    "   - Regional patterns analyzed\n",
    "\n",
    "4. **Sentiment Analysis**\n",
    "   - Overall sentiment leans positive/neutral/negative\n",
    "   - Top-rated and low-rated destinations identified\n",
    "   - Actionable insights for tourism improvement\n",
    "\n",
    "5. **Data Quality**\n",
    "   - High data completeness\n",
    "   - Minimal quality issues\n",
    "   - Ready for machine learning applications\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Feature Engineering**\n",
    "   - Create derived features from text\n",
    "   - Encode categorical variables\n",
    "   - Prepare features for modeling\n",
    "\n",
    "2. **Advanced NLP**\n",
    "   - Topic modeling (LDA)\n",
    "   - Named entity recognition\n",
    "   - Aspect-based sentiment analysis\n",
    "\n",
    "3. **Machine Learning**\n",
    "   - Build recommendation systems\n",
    "   - Predict review sentiment\n",
    "   - Cluster similar destinations\n",
    "\n",
    "4. **Deployment**\n",
    "   - Create interactive dashboards\n",
    "   - Build API endpoints\n",
    "   - Integrate with tour guide application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataframe for future use\n",
    "output_dir = '../output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "reviews_df.to_csv(f'{output_dir}/reviews_processed.csv', index=False)\n",
    "print(f\"‚úì Processed reviews saved to {output_dir}/reviews_processed.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_reviews': len(reviews_df),\n",
    "    'total_locations': len(locations_df),\n",
    "    'unique_destinations': reviews_df['destination'].nunique(),\n",
    "    'unique_districts': reviews_df['district'].nunique(),\n",
    "    'avg_review_length': reviews_df['review_length'].mean(),\n",
    "    'avg_sentiment': reviews_df['sentiment_compound'].mean(),\n",
    "    'positive_reviews_pct': (reviews_df['sentiment_label']=='Positive').sum() / len(reviews_df) * 100,\n",
    "    'data_completeness': completeness\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_stats])\n",
    "summary_df.to_csv(f'{output_dir}/summary_statistics.csv', index=False)\n",
    "print(f\"‚úì Summary statistics saved to {output_dir}/summary_statistics.csv\")\n",
    "\n",
    "print(\"\\n‚úì EDA Complete! All outputs saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close MongoDB connection\n",
    "client.close()\n",
    "print(\"‚úì MongoDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
